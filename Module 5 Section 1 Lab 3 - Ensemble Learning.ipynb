{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Section 1 Lab 1 - Ensemble Learning\n",
    "Semua metode memiliki kekurangan, seperti kurang stabil dan mudah terjebak pada kondisi *overfit* ketika digunakan pada suatu kasus tertentu yang kurang sesuai dengan karakter metode tersebut. Oleh karena itu, sejumlah peneliti mengusulkan teknik pembelajaran gabungan atau *Ensemble Learning*. \n",
    "\n",
    "## Ide dan Motivasi\n",
    "Misalkan terdapat tiga nilai prediksi harga emas untuk hari besok (naik, naik dan turun) yang dikeluarkan oleh tiga Lembaga berbeda (A, B, dan C). Ketiga lembaga tersebut membangun model prediksi menggunakan data dan teknik independen (tidak ada keterkaitan antar lembaga), di mana akurasi prediksi selama setahun terkahir adalah 80%, 82%, dan 85% secara berturut-turut untuk Lembaga A, B, dan C.\n",
    "\n",
    "Prediksi harga emas mana yang Anda bisa percaya?<br>\n",
    "Apakah Anda lebih percaya pada Lembaga C karena selama setahun ini memberikan akurasi paling tinggi sebesar 85%? Jika Ya, berarti kita yakin bahwa besok hari harga emas akan turun.\n",
    "\n",
    "Jika saya akan lebih percaya pada suara terbayak (*majority vote*) dari ketiga prediksi tersebut. Artinya, saya lebih percaya bahwa besok hari harga emas akan naik karena dua dari tiga Lembaga memprediksi naik. Jika Lembaga A, B, dan C saling independen, maka secara teori probabilitas penggabungkan ketiga model akan memberikan akurasi yang lebih besar dibandingkan dengan akurasi masing-masing model jika berdiri sendiri. Perhitungan akurasi prediksi gabungan didapatkan dari tiga kejadian, di mana dua suara 'naik' dan satu suara 'turun'. ditambah dengan satu kejadian di mana ketiga suara sama semua (mufakat).\n",
    "\n",
    "Dengan demikian, masuk akal jika lebih percaya pada suara terbayak dari ketiga Lembaga tersebut karena akurasi (peluang kebenarannya) jauh lebih besar jika dibanding akurasi masing-masing Lembaga yang berdiri sendiri.\n",
    "\n",
    "Bagaimanapun, konsep *majority voting* inihanya berlaku jika setiap model saling independen, yang dilatih dengan himpunan data yang saling terpisah. Jika setiap model tidak saling independen, makan konsep *majority voting* tidak akan bekerja.\n",
    "\n",
    "Baca selanjutnya: https://link.springer.com/chapter/10.1007/3-540-45014-9_1\n",
    "\n",
    "## Cara Kerja Ensemble Learning?\n",
    "Setiap algoritma *Ensemble Learning* terdiri dari dua langkah utama:\n",
    "1. Menghasilkan kelompok prediksi menggunakan algoritma pembelajaran sederhana.\n",
    "2. Menggabungkan prediksi menjadi satu model 'agregat'.\n",
    "\n",
    "Ensemble dapat dicapai melalui beberapa teknik.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Ensembles\n",
    "*Voting ensemble* atau dikenal dengan *majority voting ensemble* adalah model pembelajaran dengan menggabungkan prediksi dari beberapa model. Kita dapat menggunakannya untuk tugas regresi maupun klasifikasi jika memiliki dua atau lebih model yang memiliki performa baik. Dalam tugas regresi, hasil akhir didapatkan dengan menghitung rata-rata setiap prediksi model. Sedangkan dalam tugas klasifikasi, prediksi untuk setiap label dijumlahkan dan label dengan suara terbanyak diprediksi. Terdapat dua pendekatan yang dapat digunakan pada tugas klasifikasi:\n",
    "- *Hard voting* memprediksi kelas dengan jumlah suara terbanyak dari model.\n",
    "- *Soft voting* memprediksi kelas dengan probabilitas penjumlahan terbesar dari model.\n",
    "\n",
    "Baca selanjutnya: http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         0.0             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History  Loan_Status  \n",
       "0             1.0            1  \n",
       "1             1.0            0  \n",
       "2             1.0            1  \n",
       "3             1.0            1  \n",
       "4             1.0            1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/loan_prediction.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Soft Voting/Majority Rule classifier for unfitted estimators\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Buat model indiviual\n",
    "logreg_clf = LogisticRegression()\n",
    "decision_clf = DecisionTreeClassifier()\n",
    "knn_clf = KNeighborsClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Hard Voting\n",
    "voting_clf_hard = VotingClassifier(estimators=[('Logistic Regression', logreg_clf), ('Decision Tree', decision_clf), ('k-NN', knn_clf)], voting='hard')\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Performa model\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Soft Voting\n",
    "voting_clf_soft = VotingClassifier(estimators=[('Logistic Regression', logreg_clf), ('Decision Tree', decision_clf), ('k-NN', knn_clf)], voting='soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Performa model\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting accuracy: 0.6918918918918919\n",
      "Soft voting accuracy: 0.6864864864864865\n"
     ]
    }
   ],
   "source": [
    "print('Hard voting accuracy:', accuracy_hard)\n",
    "print('Soft voting accuracy:', accuracy_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) Ensemble\n",
    "Merupakan metode *Ensemble Learning* efektif yang pertama kali diusulkan. *Bagging* dikenal sebagai salah satu metode yang paling simple dalam *Arching (Adaptive Reweighting and Combining)*, sebuah terminologi umum yang mengacu pada penggunaan kembali atau pemilihan data untuk meningkatkan akurasi klasifikasi.\n",
    "\n",
    "*Bagging* membentuk *instance* yang membangun beberapa estimator pada himpunan data latih secara acak dan kemudian menggabungkan masing-masing prediksi model untuk membentuk prediksi akhir. *Bagging* menggunakan satu tipe algoritma pembelajaran dengan melibatkan manipulasi data latih dengan *resampling*. *Bagging* mempelajari $k$ algoritma pembelajaran pada $k$ sampel data latih yang berbeda.\n",
    "\n",
    "Sampel dibuat secara independen dengan melakukan sampel ulang pada data latih menggunakan bobot yang seragam. Ini berarti bahwa pengambilan sampel titik data terjadi dengan penggantian. Proses pengambilan sampel dengan penggantian disebut *Bootstrapping*.\n",
    "\n",
    "Semua *instance* dapat dilatih secara parallel melalui CPU core yang berbeda atau bahkan server yang berbeda. Demikian pula dengan prediksi dapat dibuat secara parallel. Inilah salah satu alasan mengapa metode *Bagging* sangat populer.\n",
    "\n",
    "Baca selanjutnya: https://link.springer.com/article/10.1023/A:1018054314350\n",
    "\n",
    "<center><img src='https://machinelearningmastery.com/wp-content/uploads/2020/11/Bagging-Ensemble.png' width=400></img><br><a href='https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/'>Sumber Gambar</a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting single Decision Tree\n",
    "decision_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_decision = decision_clf.predict(X_test)\n",
    "score_dt = accuracy_score(y_test, y_pred_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Fitting bagging classifier with Decision Tree\n",
    "bagging_clf = BaggingClassifier(base_estimator=decision_clf)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "score_bc_lr = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Decision Tree: 0.6702702702702703\n",
      "Bagging with Decision Tree accuracy: 0.745945945945946\n"
     ]
    }
   ],
   "source": [
    "print('Single Decision Tree:', score_dt)\n",
    "print('Bagging with Decision Tree accuracy:', score_bc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forest adalah algoritma bagging dengan pohon keputusan sebagai classifier/regressor dasar. Alih-alih mencari fitur terbaik saat memisahkan sebuah node, *random forest* mencari fitur terbaik di antara subset fitur yang acak. Ini menghasilkan keragaman pohon yang lebih besar.\n",
    "\n",
    "Baca selanjutnya: https://journals.sagepub.com/doi/full/10.1177/1536867X20909688\n",
    "\n",
    "<center><img src='https://miro.medium.com/max/700/1*5dq_1hnqkboZTcKFfwbO9A.png' width=800></img><br><a href='https://medium.com/analytics-vidhya/machine-learning-decision-trees-and-random-forest-classifiers-81422887a544'>Sumber Gambar</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on \n",
    "various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ranfor_clf_1 = RandomForestClassifier(min_samples_leaf=2)\n",
    "ranfor_clf_1.fit(X_train,y_train)\n",
    "\n",
    "ranfor_clf_2 = RandomForestClassifier(max_leaf_nodes=5)\n",
    "ranfor_clf_2.fit(X_train,y_train)\n",
    "\n",
    "# Performa\n",
    "y_pred_ranfor_1 = ranfor_clf_1.predict(X_test)\n",
    "score_ranfor_1 = accuracy_score(y_test, y_pred_ranfor_1)\n",
    "\n",
    "y_pred_ranfor_2 = ranfor_clf_2.predict(X_test)\n",
    "score_ranfor_2 = accuracy_score(y_test, y_pred_ranfor_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy with min_sample_leaf 2: 0.7405405405405405\n",
      "Random forest accuracy with max_leaf_nodes 5: 0.7567567567567568\n"
     ]
    }
   ],
   "source": [
    "print('Random forest accuracy with min_sample_leaf 2:', score_ranfor_1)\n",
    "print('Random forest accuracy with max_leaf_nodes 5:', score_ranfor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble\n",
    "Pertama kali diusulkan untuk meminimasilir tingkat kesalahan generalisas dari satu atau lebih model. *Stacking* menggabungkan beberapa model klasifikasi melalui *meta-classifier*. Ini didasarkan pada ide sederhana: alih-alih menggunakan fungsi untuk menggabungkan prediksi semua algoritma pembelajaran dalam ansambel, kita melatih model untuk melakukan agregasi ini. Singkatnya, *Stacking* memvariasikan tipe model yang sesuai dengan data latih dan menggunakan model tersebut untuk menggabungkan prediksi.\n",
    "\n",
    "*meta-classifier*: https://www.igi-global.com/dictionary/meta-classifier/45744<br>\n",
    "Baca selanjutnya: https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231\n",
    "\n",
    "<center><img src='https://machinelearningmastery.com/wp-content/uploads/2020/11/Stacking-Ensemble.png' width=400></img><br><a href='https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/'>Sumber Gambar</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('log_reg', LogisticRegression()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagging = stacking_clf.predict(X_test)\n",
    "score_sc_lr = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking accuracy: 0.7297297297297297\n"
     ]
    }
   ],
   "source": [
    "print('Stacking accuracy:', score_sc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Ensemble\n",
    "Sesuai dengan namanya, metode *Boosting* bekerja dengan cara memperkuat (*boost*) sebuah model pengklasifikasi. *Boosting* muncul dari ide apakah algoritma pembelajaran yang lemah dapat dimodifikasi secara berurutan menjadi lebih baik. *Boosting* mengacu pada metode Ensemble apa pun yang dapat menggabungkan algoritma pembelajaran yang lemah menjadi model yang kuat. Ide umum dari sebagian besar metode ini adalah melatih prediktor secara berurutan, masing-masing mencoba memperbaiki pendahulunya. Proses penguatan model secara sekuensial dilakukan sebanyak $T$ kali sampai dihasilkan model yang cukup kuat. Sejumlah $T$ model yang dihasilkan selanjutnya digabungkan menggunakan *majority vote* dengan pembobotan sesuai performanya.\n",
    "\n",
    "Baca selanjutnya: https://www-ai.cs.tu-dortmund.de/LEHRE/PG/PG445/literatur/schapire_99a.pdf\n",
    "\n",
    "<center><img src='https://machinelearningmastery.com/wp-content/uploads/2020/11/Boosting-Ensemble.png' width=400></img><br><a href='https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/'>Sumber Gambar</a></center>\n",
    "\n",
    "Ada banyak jenis *Boosting*, namun yang paling populer adalah:\n",
    "* AdaBoost (Adaptive Boosting)\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Baca selanjutnya: https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "score_ada = accuracy_score(y_test, y_pred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy: 0.745945945945946\n"
     ]
    }
   ],
   "source": [
    "print('AdaBoost accuracy:', score_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Baca selanjutnya: https://statweb.stanford.edu/~jhf/ftp/trebst.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, min_samples_leaf=10, max_depth=1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "score_gb = accuracy_score(y_test, y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting accuracy: 0.7567567567567568\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Boosting accuracy:', score_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
